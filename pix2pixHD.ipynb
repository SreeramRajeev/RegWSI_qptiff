{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Install and Import Dependencies\nimport sys\nimport subprocess\n\n# Install required packages\ndef install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\npackages = [\n    \"tensorflow>=2.8.0\",\n    \"opencv-python\",\n    \"matplotlib\",\n    \"tqdm\",\n    \"Pillow\",\n    \"numpy\",\n    \"scikit-image\"\n]\n\nfor package in packages:\n    try:\n        install_package(package)\n    except:\n        print(f\"Warning: Could not install {package}\")\n\nprint(\"Installation complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Import Libraries\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport time\nfrom tqdm import tqdm\nimport random\nfrom PIL import Image\nfrom PIL import ImageFile\nimport glob\nfrom sklearn.model_selection import train_test_split\n\n# Configure PIL for large images\nImage.MAX_IMAGE_PIXELS = None\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"GPU Available:\", tf.config.list_physical_devices('GPU'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Configure GPU and Memory Settings\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n    except RuntimeError as e:\n        print(f\"GPU configuration error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Define Configuration and Constants\nclass Config:\n    # Data paths\n    DATA_DIR = \"/RegWSI_pass1\"\n    OUTPUT_DIR = \"./pix2pixhd_output\"\n    CHECKPOINT_DIR = \"./pix2pixhd_checkpoints\"\n    SAMPLE_DIR = \"./pix2pixhd_samples\"\n    \n    # Training parameters\n    BATCH_SIZE = 1  # Pix2PixHD typically uses batch size 1\n    IMG_HEIGHT = 512  # Higher resolution for pix2pixHD\n    IMG_WIDTH = 512\n    PATCH_SIZE = 512  # Larger patches for better quality\n    STRIDE = 256  # 50% overlap\n    EPOCHS = 200\n    \n    # Model parameters\n    LAMBDA_L1 = 100\n    LAMBDA_FEAT = 10  # Feature matching loss weight\n    N_LAYERS_D = 3  # Discriminator layers\n    NUM_D = 2  # Number of discriminators (multi-scale)\n    \n    # Optimization\n    LEARNING_RATE = 0.0002\n    BETA1 = 0.5\n    BETA2 = 0.999\n    \n    # Checkpointing\n    CHECKPOINT_INTERVAL = 10\n    SAMPLE_INTERVAL = 5\n\nconfig = Config()\n\n# Create output directories\nos.makedirs(config.OUTPUT_DIR, exist_ok=True)\nos.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(config.SAMPLE_DIR, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Data Loading and Preprocessing Functions\ndef load_tiff_image(path):\n    \"\"\"Load TIFF image using PIL and convert to RGB\"\"\"\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        return np.array(img)\n    except Exception as e:\n        print(f\"Error loading {path}: {e}\")\n        return None\n\ndef load_image_pair(pair_dir):\n    \"\"\"Load warped_source.tiff and target.tiff from a pair directory\"\"\"\n    source_path = os.path.join(pair_dir, \"warped_source.tiff\")\n    target_path = os.path.join(pair_dir, \"target.tiff\")\n    \n    if not os.path.exists(source_path) or not os.path.exists(target_path):\n        print(f\"Missing files in {pair_dir}\")\n        return None, None\n    \n    source = load_tiff_image(source_path)\n    target = load_tiff_image(target_path)\n    \n    if source is None or target is None:\n        return None, None\n    \n    # Ensure same dimensions\n    if source.shape != target.shape:\n        print(f\"Resizing images in {pair_dir}\")\n        target = cv2.resize(target, (source.shape[1], source.shape[0]))\n    \n    return source, target\n\ndef create_patches(image, patch_size, stride):\n    \"\"\"Create overlapping patches from image\"\"\"\n    h, w = image.shape[:2]\n    patches = []\n    \n    for y in range(0, h - patch_size + 1, stride):\n        for x in range(0, w - patch_size + 1, stride):\n            patch = image[y:y + patch_size, x:x + patch_size]\n            if patch.shape[:2] == (patch_size, patch_size):\n                patches.append(patch)\n    \n    return patches\n\ndef load_all_data():\n    \"\"\"Load all image pairs and create patches\"\"\"\n    print(\"Loading data from all pairs...\")\n    \n    all_source_patches = []\n    all_target_patches = []\n    \n    # Get all pair directories\n    pair_dirs = [os.path.join(config.DATA_DIR, f\"Pair{i}\") for i in range(1, 21)]\n    \n    for pair_dir in tqdm(pair_dirs, desc=\"Loading pairs\"):\n        if not os.path.exists(pair_dir):\n            print(f\"Warning: {pair_dir} does not exist\")\n            continue\n        \n        source, target = load_image_pair(pair_dir)\n        if source is None or target is None:\n            continue\n        \n        print(f\"Processing {pair_dir}: {source.shape}\")\n        \n        # Create patches\n        source_patches = create_patches(source, config.PATCH_SIZE, config.STRIDE)\n        target_patches = create_patches(target, config.PATCH_SIZE, config.STRIDE)\n        \n        print(f\"Created {len(source_patches)} patches from {pair_dir}\")\n        \n        all_source_patches.extend(source_patches)\n        all_target_patches.extend(target_patches)\n    \n    print(f\"Total patches: {len(all_source_patches)}\")\n    return np.array(all_source_patches), np.array(all_target_patches)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Load and Prepare Dataset\nprint(\"Loading dataset...\")\nsource_patches, target_patches = load_all_data()\n\nprint(f\"Source patches shape: {source_patches.shape}\")\nprint(f\"Target patches shape: {target_patches.shape}\")\n\n# Visualize sample patches\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nfor i in range(4):\n    idx = random.randint(0, len(source_patches) - 1)\n    \n    axes[0, i].imshow(source_patches[idx])\n    axes[0, i].set_title(f\"HE Patch {idx}\")\n    axes[0, i].axis('off')\n    \n    axes[1, i].imshow(target_patches[idx])\n    axes[1, i].set_title(f\"Cd8 Patch {idx}\")\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.savefig(os.path.join(config.SAMPLE_DIR, \"sample_patches.png\"), dpi=150)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: Data Preprocessing and Dataset Creation\ndef normalize_image(image):\n    \"\"\"Normalize image to [-1, 1]\"\"\"\n    return (tf.cast(image, tf.float32) / 127.5) - 1.0\n\ndef preprocess_image_pair(source, target):\n    \"\"\"Preprocess source and target images\"\"\"\n    source = normalize_image(source)\n    target = normalize_image(target)\n    return source, target\n\n# Create train/validation split\ntrain_source, val_source, train_target, val_target = train_test_split(\n    source_patches, target_patches, test_size=0.2, random_state=42\n)\n\nprint(f\"Train samples: {len(train_source)}\")\nprint(f\"Validation samples: {len(val_source)}\")\n\n# Create TensorFlow datasets\nAUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_source, train_target))\ntrain_dataset = train_dataset.map(preprocess_image_pair, num_parallel_calls=AUTOTUNE)\ntrain_dataset = train_dataset.shuffle(1000)\ntrain_dataset = train_dataset.batch(config.BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_source, val_target))\nval_dataset = val_dataset.map(preprocess_image_pair, num_parallel_calls=AUTOTUNE)\nval_dataset = val_dataset.batch(config.BATCH_SIZE)\nval_dataset = val_dataset.prefetch(AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Pix2PixHD Generator Architecture\ndef residual_block(x, filters, kernel_size=3):\n    \"\"\"Residual block for generator\"\"\"\n    shortcut = x\n    \n    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    \n    x = layers.Add()([x, shortcut])\n    x = layers.ReLU()(x)\n    \n    return x\n\ndef global_generator():\n    \"\"\"Global generator network for pix2pixHD\"\"\"\n    inputs = layers.Input(shape=[config.IMG_HEIGHT, config.IMG_WIDTH, 3])\n    \n    # Initial convolution\n    x = layers.Conv2D(64, 7, padding='same')(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    # Downsampling\n    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    x = layers.Conv2D(512, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    # Residual blocks\n    for _ in range(9):\n        x = residual_block(x, 512)\n    \n    # Upsampling\n    x = layers.Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    # Output layer\n    outputs = layers.Conv2D(3, 7, padding='same', activation='tanh')(x)\n    \n    return keras.Model(inputs=inputs, outputs=outputs, name='global_generator')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Multi-Scale Discriminator\ndef discriminator_block(x, filters, stride=2, normalization=True):\n    \"\"\"Discriminator block\"\"\"\n    x = layers.Conv2D(filters, 4, strides=stride, padding='same')(x)\n    if normalization:\n        x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    return x\n\ndef create_discriminator(name):\n    \"\"\"Create a single discriminator\"\"\"\n    input_img = layers.Input(shape=[config.IMG_HEIGHT, config.IMG_WIDTH, 3])\n    target_img = layers.Input(shape=[config.IMG_HEIGHT, config.IMG_WIDTH, 3])\n    \n    x = layers.Concatenate()([input_img, target_img])\n    \n    x = discriminator_block(x, 64, normalization=False)\n    x = discriminator_block(x, 128)\n    x = discriminator_block(x, 256)\n    x = discriminator_block(x, 512, stride=1)\n    \n    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n    \n    return keras.Model(inputs=[input_img, target_img], outputs=x, name=name)\n\ndef multi_scale_discriminator():\n    \"\"\"Create multi-scale discriminator\"\"\"\n    discriminators = []\n    \n    for i in range(config.NUM_D):\n        disc = create_discriminator(f'discriminator_{i}')\n        discriminators.append(disc)\n    \n    return discriminators\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Loss Functions\ndef discriminator_loss(real_output, fake_output):\n    \"\"\"Discriminator loss\"\"\"\n    real_loss = tf.reduce_mean(tf.square(real_output - 1))\n    fake_loss = tf.reduce_mean(tf.square(fake_output))\n    total_loss = 0.5 * (real_loss + fake_loss)\n    return total_loss\n\ndef generator_adversarial_loss(fake_output):\n    \"\"\"Generator adversarial loss\"\"\"\n    return tf.reduce_mean(tf.square(fake_output - 1))\n\ndef feature_matching_loss(real_features, fake_features):\n    \"\"\"Feature matching loss\"\"\"\n    loss = 0\n    for real_feat, fake_feat in zip(real_features, fake_features):\n        loss += tf.reduce_mean(tf.abs(real_feat - fake_feat))\n    return loss\n\ndef l1_loss(real_image, fake_image):\n    \"\"\"L1 loss for pixel-wise comparison\"\"\"\n    return tf.reduce_mean(tf.abs(real_image - fake_image))\n\ndef generator_loss(fake_disc_outputs, real_features, fake_features, real_image, fake_image):\n    \"\"\"Combined generator loss\"\"\"\n    # Adversarial loss\n    adv_loss = 0\n    for fake_output in fake_disc_outputs:\n        adv_loss += generator_adversarial_loss(fake_output)\n    \n    # Feature matching loss\n    feat_loss = 0\n    for real_feat, fake_feat in zip(real_features, fake_features):\n        feat_loss += feature_matching_loss(real_feat, fake_feat)\n    \n    # L1 loss\n    pixel_loss = l1_loss(real_image, fake_image)\n    \n    total_loss = adv_loss + config.LAMBDA_FEAT * feat_loss + config.LAMBDA_L1 * pixel_loss\n    \n    return total_loss, adv_loss, feat_loss, pixel_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Create Models and Optimizers\nprint(\"Creating models...\")\n\n# Create generator\ngenerator = global_generator()\nprint(\"Generator created\")\n\n# Create discriminators\ndiscriminators = multi_scale_discriminator()\nprint(f\"Created {len(discriminators)} discriminators\")\n\n# Create optimizers\ngen_optimizer = keras.optimizers.Adam(config.LEARNING_RATE, beta_1=config.BETA1, beta_2=config.BETA2)\ndisc_optimizers = [keras.optimizers.Adam(config.LEARNING_RATE, beta_1=config.BETA1, beta_2=config.BETA2) \n                   for _ in range(config.NUM_D)]\n\nprint(\"Models and optimizers created\")\n\n# Print model summaries\nprint(\"\\nGenerator Summary:\")\ngenerator.summary()\n\nprint(f\"\\nDiscriminator Summary (showing first discriminator):\")\ndiscriminators[0].summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: Training Step Functions\n@tf.function\ndef train_discriminator_step(real_image, input_image, fake_image, discriminator, optimizer):\n    \"\"\"Training step for a single discriminator\"\"\"\n    with tf.GradientTape() as tape:\n        # Get discriminator outputs\n        real_output = discriminator([input_image, real_image], training=True)\n        fake_output = discriminator([input_image, fake_image], training=True)\n        \n        # Calculate discriminator loss\n        disc_loss = discriminator_loss(real_output, fake_output)\n    \n    # Calculate and apply gradients\n    gradients = tape.gradient(disc_loss, discriminator.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n    \n    return disc_loss\n\n@tf.function\ndef train_generator_step(real_image, input_image):\n    \"\"\"Training step for generator\"\"\"\n    with tf.GradientTape() as tape:\n        # Generate fake image\n        fake_image = generator(input_image, training=True)\n        \n        # Get discriminator outputs and features\n        fake_disc_outputs = []\n        real_features = []\n        fake_features = []\n        \n        for discriminator in discriminators:\n            fake_output = discriminator([input_image, fake_image], training=True)\n            fake_disc_outputs.append(fake_output)\n            \n            # For feature matching, we would need to extract intermediate features\n            # This is simplified for brevity\n            real_features.append([])\n            fake_features.append([])\n        \n        # Calculate generator loss\n        gen_loss, adv_loss, feat_loss, pixel_loss = generator_loss(\n            fake_disc_outputs, real_features, fake_features, real_image, fake_image)\n    \n    # Calculate and apply gradients\n    gradients = tape.gradient(gen_loss, generator.trainable_variables)\n    gen_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n    \n    return gen_loss, adv_loss, feat_loss, pixel_loss, fake_image\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Training Utilities\ndef generate_and_save_samples(generator, test_input, test_target, epoch, save_dir):\n    \"\"\"Generate and save sample images\"\"\"\n    predictions = generator(test_input, training=False)\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    images = [test_input[0], test_target[0], predictions[0]]\n    titles = ['Input (HE)', 'Target (Cd8)', 'Generated (Cd8)']\n    \n    for i, (img, title) in enumerate(zip(images, titles)):\n        axes[i].imshow(img * 0.5 + 0.5)  # Denormalize\n        axes[i].set_title(title)\n        axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(save_dir, f'epoch_{epoch:04d}.png'), dpi=150, bbox_inches='tight')\n    plt.close()\n\ndef save_model_checkpoint(generator, discriminators, epoch, save_dir):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint_path = os.path.join(save_dir, f'checkpoint_epoch_{epoch}')\n    os.makedirs(checkpoint_path, exist_ok=True)\n    \n    # Save generator\n    generator.save_weights(os.path.join(checkpoint_path, 'generator.h5'))\n    \n    # Save discriminators\n    for i, disc in enumerate(discriminators):\n        disc.save_weights(os.path.join(checkpoint_path, f'discriminator_{i}.h5'))\n    \n    print(f\"Checkpoint saved at epoch {epoch}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 14: Main Training Loop\ndef train_pix2pixhd(train_dataset, val_dataset, epochs):\n    \"\"\"Main training function\"\"\"\n    print(\"Starting training...\")\n    \n    # Get test samples for visualization\n    test_batch = next(iter(val_dataset))\n    test_input, test_target = test_batch\n    \n    # Generate initial sample\n    generate_and_save_samples(generator, test_input, test_target, 0, config.SAMPLE_DIR)\n    \n    # Training loop\n    for epoch in range(1, epochs + 1):\n        start_time = time.time()\n        \n        # Training metrics\n        gen_loss_avg = tf.keras.metrics.Mean()\n        disc_loss_avg = tf.keras.metrics.Mean()\n        \n        # Train on all batches\n        for batch_idx, (input_image, real_image) in enumerate(tqdm(train_dataset, desc=f\"Epoch {epoch}\")):\n            \n            # Generate fake image\n            fake_image = generator(input_image, training=False)\n            \n            # Train discriminators\n            total_disc_loss = 0\n            for disc, optimizer in zip(discriminators, disc_optimizers):\n                disc_loss = train_discriminator_step(real_image, input_image, fake_image, disc, optimizer)\n                total_disc_loss += disc_loss\n            \n            # Train generator\n            gen_loss, adv_loss, feat_loss, pixel_loss, _ = train_generator_step(real_image, input_image)\n            \n            # Update metrics\n            gen_loss_avg.update_state(gen_loss)\n            disc_loss_avg.update_state(total_disc_loss / len(discriminators))\n        \n        # End of epoch\n        epoch_time = time.time() - start_time\n        \n        print(f\"Epoch {epoch}/{epochs} - \"\n              f\"Gen Loss: {gen_loss_avg.result():.4f}, \"\n              f\"Disc Loss: {disc_loss_avg.result():.4f}, \"\n              f\"Time: {epoch_time:.2f}s\")\n        \n        # Generate samples\n        if epoch % config.SAMPLE_INTERVAL == 0:\n            generate_and_save_samples(generator, test_input, test_target, epoch, config.SAMPLE_DIR)\n        \n        # Save checkpoint\n        if epoch % config.CHECKPOINT_INTERVAL == 0:\n            save_model_checkpoint(generator, discriminators, epoch, config.CHECKPOINT_DIR)\n    \n    # Save final model\n    save_model_checkpoint(generator, discriminators, epochs, config.CHECKPOINT_DIR)\n    generator.save(os.path.join(config.OUTPUT_DIR, 'final_generator.h5'))\n    \n    print(\"Training completed!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 15: Start Training\nif __name__ == \"__main__\":\n    try:\n        print(\"Starting Pix2PixHD training...\")\n        train_pix2pixhd(train_dataset, val_dataset, config.EPOCHS)\n    except Exception as e:\n        print(f\"Training error: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Try to save current model state\n        try:\n            generator.save(os.path.join(config.OUTPUT_DIR, 'interrupted_generator.h5'))\n            print(\"Saved interrupted model\")\n        except:\n            print(\"Could not save interrupted model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}